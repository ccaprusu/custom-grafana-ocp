== Brett's Notes from visit on 12/2-12/5 2019

- Access:
* Console:
gocp-master.apria.com
gocp-master-np.apria.com
* SSH:
ocp-bastion.apria.com
ocp-bastion-np.apria.com
apria-admin
* Access to the GitHub repo for Jenkins - APPS-JSS-CICD - has been provided
* Access to the GitHub repo for OCP infra - ocp-ansible - has been provided
* Access to AWS infrastructure environment in US-West 2 has been provided

- Notes for Craig -
* Check LB configuration in AWS - it doesn’t seem to be bouncing between masters correctly
* Registry is using a self-signed cert
* AMQ 63 installed in registry, plus an Apria custom AMQ image - what is this?
* Patient-notification-configmap - amq credentials in cleartext. (Multiple instances in multiple config maps)
* Hanging PVC - routing-sac
* Should at least upgrade to AMQ63 v.1.4, they are on a 1.2 image right now
* Imagestream for resmed-poll-patients in apria-amq project (it’s empty but an artifact)
* Apria-amq project has two amq images, not sure what the difference is (one is labeled custom)

- Gluster - historically needed to provide RWX for AMQ and Hippa compliance
* Still needs a 4th node - not a lot of data currently being stored on the cluster
* Gluster nodes are paired with the infra nodes - check to see how the cluster is configured and spread across the instances
* Fault tolerance is the big claim for AMQs need of Gluster, but they are not running more than a single AMQ instance (not multiple brokers, or master/slave deployments) - I’m not sure this is even relevant inside OCP, but if it were, they would need affinity/taints/tolerance settings to balance brokers across the various nodes
* The proper way to install AMQ on OpenShift is with multiple brokers each with their own PVC

- ClearData
* ClearData AMI’s w/ Salt setup - AMIs provided and they are looking at a monthly rollout to capture more recent updates
* Might be issues with OCP 4 on that
* Will use this for Tower

- OCP Platform Version
* Sticking with 3.11 for now and get it into a supportable state - then in January maybe look at OCP 4 migration
* Roadblocks are AMQ mostly due to version not being supported - apps would need to be updated to move

- OCP Install Issues
* Internal routes are currently setup manually to route from on-site to AWS, but they want to setup a forwarding domain in InfoBlocks to AWS Route53
* They have some A records (short names) to redirect to the FQDN in AWS.
* Logging - Kibana has OAuth errors - just needed to be restarted
* Non-prod has no alerts setup via alert manager
* Take a look at the /etcd/ setup on the masters - Antonio thought something looked odd

- Applications
* They have some back end data lake services where incoming data gets stored on premises.  Their Kaiser services will act a go between between data stores.
* They are running all services in the same namespace - access control issues with data?  Need to find a better design.

- Operations
* Patching - ClearData using EPEL for it’s Salt updates, they have created a playbook for the RHEL patching
* ClearData is doing a semi-MSP for AWS environments - compliance and operations issues are monitored and enforced
* Internal IT operations are being transitioned between NTTData and Infosys, but they own a nearly full suite of Solarwinds subs
* They would like to pull AWS logs into Solarwinds at some point
* Alerts are all email based - I found no evidence that alerting had been setup for OCP via Alert Manager
OCP Alerting not configured, but the devs are pretty aggressive when things go wrong - not really in production, they’ve been in development for 2 years or so.  
* They are doing kernel patching but no direct OCP upgrades - need to build a model for doing both.  They are currently on hold for this due to the MSP handover.
* AWS LB configuration - no wildcard endpoints allowed from external, they go through a network LB and an application LB and both need to be configured when a route is exposed - NLB forward to TG which forward to fixed IPs - this is not ideal - Lamdba function pings every 5 minutes to update the Target group with updated IPs
* Route53 has IP to hostname mappings for convenience - they are not used in general and the IP addresses require manual updating - we should not rely on this in any of our automation and should always reference nodes by AWS host name

- Devops
* No real DevOps pipelines outside of what RH setup.  The deployments are questionable.
* Devs have same level of access in non-prod & prod - fewer in prod than non-prod, managed via LDAP groups
* They have no cloud formation scripts today to rebuild the clusters, it must be done by hand - they do have their inventory in GitHub to rebuild from there - the AWS environment with security groups etc are going to be the biggest hurdles to recreate their complex LB configuration complicates this

- Backups
* ClearData takes snapshots of all the EC2 instances daily
* Inventory is calling out specific AWS instance names which causes issues with IP mismatch during recovery

- Ticketing
* They don’t use a ticketing system ATM, but are transitioning between providers and are exploring their options.  This will be an issue with them if they can’t provide something.  ServiceNow is on the table with the new provider but nothing is set in stone at the moment.  When will ticketing be available is the question.  How will this affect our SLA delivery and proof of action?

- Other
* RH subs renewal in April - Currently Gluster (2 core) Qty 8 is available for Prod and Qty 12 for non-prod/labs
* User KMOOS has single user cluster admin privileges - RH Architect

- Resolved
* No Tower subs - they might want to setup a temp license until procurement can get through the SOW (comparing pricing right now)
* Send Craig the Requirements for Tower - DONE
* Need a new GitHub repo for our MSP Ansible plays to integrate with Tower
